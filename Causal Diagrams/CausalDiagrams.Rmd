---
title: "Causality"
author: "Christoph Hanck"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    css: ["default", "../assets/ude_fonts.css", "../assets/ude.css",  "../assets/custom.css"]
    chakra: '../libs/remark-latest.min.js'
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "../assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["../assets/remark-zoom.js", '../libs/local.js']
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' #alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false #disable slide transitions by scrolling
---

```{r, load_refs, include = FALSE, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'authoryear', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
myBib <- ReadBib("../assets/example.bib", check = FALSE)

# packages
source('../assets/packages.R')

library(kableExtra)

counter <- function() {
  i <- 0
  function() {
    i <<- i + 1
    return(i)
  }
}

tbl_counter <- counter()
fig_counter <- counter()

# transparent images
library(tidyverse)
library(lubridate)
theme_set(theme_bw(base_size=12))
theme_update(panel.background = element_rect(fill = "transparent", colour = NA),
             plot.background = element_rect(fill = "transparent", colour = NA))

opts_chunk$set(
  dev.args=list(bg="transparent"),
  message = F, echo = F
  )
```

```{r xaringanExtra_progress-bar, echo = FALSE}
xaringanExtra::use_progress_bar(color = "#004c93", location = "bottom")
```

```{r xaringanExtra-clipboard_2, echo=FALSE}
# copy button styles mainly in ude.css 
# https://github.com/gadenbuie/xaringanExtra
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #00ff00\"></i>",
    error_text = "<i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

##Motivation

Before going into all the technicalities of causality, we need to understand *why* we should worry so much about causality: **many interesting research questions are causal in nature.**

<br>

.blockquote[

###Examples: Economic research questions

- We don’t want to know if countries with higher minimum wages have less poverty, we want to know if raising the minimum wage reduces poverty. 
- We don’t want to know if people who take a popular common-cold-shortening medicine get better, we want to know if the medicine made them get better more quickly. 
- We don’t want to know if the central bank cutting interest rates was shortly followed by a recession, we want to know if the interest rate cut caused the recession.

]

---
## So What is Causality?

.vcenter[
.blockquote[

###Definition: Causality

We say that $X$ causes $Y$ when we interfere and change the value of $X$ without changing anything else, then the distribution of $Y$ would also change as a result.

]]


---
## Causality &mdash; Examples

.vcenter[
.blockquote[
### A few examples of causality

Obvious ones:
 - A light switch being set to on causes the light to be on
 - Setting off fireworks raises the noise level

Less obvious ones:
 - Getting a college degree increases your earnings
 - Tariffs reduce the amount of trade
]]

---
## Causality 

**Correlation does not imply causality.**

Often we observe non-zero correlations that do not reflect causal relationships (or may be causal in the wrong direction!)

.blockquote[
###Examples: causality vs. correlation

Obvious ones:
- People tend to wear shorts on days when ice cream trucks are out
- Rooster crowing sounds are followed closely by sunrise

Less obvious ones:
- Colds tend to clear up a few days after you take Emergen-C
- The performance of the economy tends to be lower or higher depending on the president’s political party
]

---
##Casusality 

Not all $Y$ must be due to $X$: we still say $X$ causes $Y$ even if changing $X$ does not always change $Y$, but just changes the distribution of $Y$: the **probability** that $Y$ occurs. 

<br>

.blockquote[
###Example: switches and bulbs

Flipping a light switch ( $X$ ) on alters the distribution of the light coming on ( $Y$ ), but not that it  happens for certain:

- The bulb may be broken

- The bulb may be loose

- There may be no electricity

]

---
##Weasel words

There are lots of words that are generally taken to imply causality, as well as words that describe relationships *without* implying causality. 

There are also some “weasel words” that don’t technically say anything about causality but clearly really want you to hear it.


<br>
.center[What are some of these words?]
</br>


We can say $X$ causes $Y$ by saying:
- $X$ causes $Y$
- $X$ affects $Y$
- The effect of $X$ on $Y$
- ...

---
##Why are Weasel words important?

- Knowing these terms can help interpreting what scientific studies are really saying, and when someone might be trying to be misleading:

  Weasel terms are problematic because they convey that there is a relation between $X$ and $Y$ (from $X$ to $Y$), without literally claiming a causal relationship.

- Clearly non-causal terms do not even specify which of $X$ and $Y$ goes first! They just talk about these two variables and how they work together. 

- Causal phrases always have a clear direction. They further tell us *what* $X$ is doing to $Y$.


.footnote[
The key is that all the weasel phrases are equally true if you swap the positions of X and Y, even though swapping them would really change how we’d interpret the claim. “Aspirin is linked to headaches” is no more or less true than “Headaches are linked to aspirin.” You can hear the notion that aspirin causes your headache in the first one, which is false. You can get away with it without technically lying because it’s true that they’re “linked.”
]

---
##The Problem of Causal Inference
**Potential outcomes**

Meet Angela with health status $Y$ and consider the variable

$$\begin{equation*} X = \begin{cases}1, \ \text{person gets medical treatment}\\ 0, \ \text{else.} \end{cases} \end{equation*}$$

- We we would like to check Angela’s $Y$ when choosing $X=0$ and also check what Angela’s $Y$ is again when we make $X=1$. If the two $Y$'s are different, then we can say that $X$ causes $Y$.

- However, Angela cannot exist both with $X=0$ and with $X=1$. She either got that medical treatment or she did not!

- Let’s say she did. So for Angela, X=1 and, let’s say, Y=10.
The other one, what Y would have been if we made X=0, is missing. We don’t know what it is! Could also be Y=10. Could be Y=9. Could be Y=1000!

---
##A possible solution!

- We can take someone who actually DOES have X=0 and compare their Y?

- However, as both of them are different people, it is quite natural that they will have different Y BESIDES X. 

- So if we find someone, Gareth, with X=0 and they have Y=9, is that because X increases Y, or is that just because Angela and Gareth would have had different Ys anyway?

- In order to overcome with this problem, and making as good a guess as possible as to what that Y would have been if X had been different, we want to think about two people/firms/countries that are basically **exactly the same** except that one has X=0 and one has X=1
</br>

---
##POTENTIAL OUTCOMES
<br>
The logic we just went through is the basis of the potential outcomes model, which is one way of thinking about causality.
</br>
<br>
Figuring out that makes a good counterfactual estimate is a key part of causal inference!
</br>
.footnote[
counterfactual - counter to the fact of what actually happened]

---
##Experiments and Models
- A common way to do causal inference in many fields is an experiment.
- When we’re working with people/firms/countries, running experiments is often infeasible, impossible, or unethical.
- So we have to think hard about a model of what the world looks like.
- So that we can use our model to figure out what the counterfactual would be.
- In causal inference, the model is our idea of what we think the process is that generated the data.
- We put together what we know about the world with assumptions we make and end up with our model.
-The model can then tell us what kinds of things could give us wrong results so we can fix them and get the right counterfactual.

---
#Model Identification
- We have “identified” a causal effect if the estimate that we generate gives us a causal effect.
- In other words, when we see the estimate, we can claim that it’s isolating just the causal effect.
- Identifying effects requires us to understand the data generating process (DGP).
- And once we understand that DGP, knowing what calculations we need to do to isolate our effect.Often these will require taking some conditional values (controls). Or isolating the variation we want in some otherway.
- We need to think about how to create models of the processes that generated the data.
- Once we have those models, we’ll figure out what methods we can use to generate plausible counterfactuals.
- Once we’re really comparing apples to apples, we can figure out, using only data we can actually observe, how things would be different if we reached in and changed X, and how Y would change as a result.

---
##Causal Diagrams

.vcenter[
.blockquote[
### Definition Causal Diagram
A causal diagram is a graphical representation of a data generating process (DGP).
]]

---
##Causal Diagrams

- Causal diagrams were developed in the mid-1990s by the computer scientist Judea Pearl (2009Pearl, Judea. 2009. Causality. 2nd ed. Cambridge, MA: Cambridge University Press.), who was trying to develop a way for artificial intelligence to think about causality. 

- A causal diagram contains only two things:
 - The variables in the DGP, each represented by a node on the diagram.
 - The causal relationships in the DGP, each represented by an arrow from the cause variable to the caused variable.

---
##A simple Example
- Two people flips a coin. 
- If they get head, then person A gets a slice of cake. 
- If thex get tail, then person B gets the slice.
- Here, we have two variables 
 - the outcome of coin flip
 - which person gets the cake
- We have one causal relationship
 - the outcome of the coin flip determines which person got cake.
`
```{r, include=FALSE}
library(ggdag)
library(ggplot2)

```
 
```{r, eval=TRUE,echo=FALSE,fig.height=2}
coord_dag <- list(
  x = c(a = 3, b = 1),
  y = c(a = 1, b = 1)
)
dag_object <- ggdag::dagify(a ~ b,
                            
                            coords = coord_dag,
                            labels=c("a"="Cake",
                                     "b"="Coin flip"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()

```
 
---
##Points to be noted:
- Each variable on the graph may take multiple values. 

- The arrow just tells us that one variable causes another. It doesn’t say anything about whether that causal effect is positive or negative.

- Other events that might effect the outcomes are ignores.

- Outcomes of outcomes are not taken into consideration.

- All (non-trivial) variables relevant to the data generating process should be included, even if we can’t measure or see them.

---
###Unmeasured Variables
- Serve two purposes in causal diagrams:
 - they are the key parts of the data generating process
 - hey can sometimes fill in for variables that we know must be there, but we have no idea what they are.


###Latent variables
- They are an example of unmeasured variables.
- Explains why two variables that are correlated but neither of them causes the other. 
- We can put them in diagram lebelled as "L" or "U".

---
##An Example:

When people wear Shorts, there is a high sale of Ice-Cream. However, there is no correlation between , sales of Ice-cream and people wearing sorts.A latent variable doing causing them both.

```{r, eval=TRUE,echo=FALSE,fig.height=3}
coord_dag <- list(
  x = c(a = 1, b = 3, c = 5),
  y = c(a = 1, b = 2, c = 1)
)
dag_object <- ggdag::dagify(a ~ b,
                            c ~ b,
                            coords = coord_dag,
                            labels=c("a"="Shorts",
                                     "b"="U",
                                     "c"="Ice-Cream"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```


---
```{r,include=FALSE}
library(wooldridge)
library(tidyverse)
library(extrafont)
library(ggpubr)
library(ggplot2)
```
##The Real World: On an Omission Mission
.pull-left[
- The following figure shows: Police Presence and Crime Rate by County, North Carolina 1981-1987.
- The graph implies: Additional police presence is consistently associated with more crime.
- The raw correlation is picking up the fact that high-crime areas get assigned lots of police.]

]

.pull-right[
```{r, eval=TRUE,echo=FALSE,fig.height=3}
data(crime4)
ggplot(crime4, aes(x = polpc, y = crmrte)) + 
  geom_point() + 
  theme_pubr() + 
  scale_x_log10() + 
  scale_y_log10() + 
  theme(text         = element_text(size = 13 ),
        axis.title.x = element_text(size = 13 ),
        axis.title.y = element_text(size = 13)) +
  labs(x = "per Capita Police (log scale)",
       y = "Crime Rate (log scale)")
```
]

---
##Causal diagram

- To make a causal diagram, we think of following latent varaibles:
 - The unobservable ExpectedCrimePayout
 - It is based on how many police there are and sentencing law
- Sentencing laws and police per capita are also caused by LawAndOrderPolitics
- Some assumptions are:
 - LaggedCrime doesn’t cause LawAndOrderPolitics
 - PovertyRate isn’t a part of the data generating process
 - LaggedPolicePerCapita doesn’t cause PolicePerCapita 
 - RecentPopularCrimeMovie doesn’t cause Crime 

---
##The Diagram:
The causal diagram looks like:


```{r, eval=TRUE,echo=FALSE,fig.height=6}
coord_dag <- list(
  x = c(a = 1, b = 4, c = 4, d = 7, e = 1, f = 7),
  y = c(a = 1, b = 5, c = 3, d = 1, e = 5, f = 5)
)
dag_object <- ggdag::dagify(b ~ a,
                            c ~ a,
                            d ~ a,
                            d ~ c,
                            f ~ c,
                            c ~ e,
                            b ~ e,
                            f ~ b,
                            f ~ d,
                            coords = coord_dag,
                            labels=c("a"="Lagged crime",
                                     "b"="Sentencing Laws",
                                     "c"="Police per Capita",
                                     "d"="Crime",
                                     "e"="Law and Order politics",
                                     "f"="Expected Crime Payouts"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```
]
---
##Research Questions in Causal Diagrams
- We should use our causal diagram to figure out how to identify the answer to our research question.

For instance, let us consider the police-and-crime figure again.

**Question** “does additional police presence reduce crime?”



- Looking at this diagram, we should be able to see which parts of the diagram answer our research question of interest.(any parts of it that allow PolicePerCapita to cause Crime)
- We have:
 - PolicePerCapita → Crime (direct effect) andExpectedCrimePayout → Crime (indirect effect)
- Ask yourself: if the reason that police presence reduces crime is because criminals commit less crime when they expect to be caught 
- The variation in our data that answers our research question has to do with PolicePerCapita causing Crime, and to do with PolicePerCapita causing ExpectedCrimePayout, which then affects Crime. 
- To identify our answer, we have to dig out that part of the variation and block out the alternative explanations.

---
##Moderators in Causal Diagrams
.content-box-red[
Moderators are variables that don’t necessarily cause another variable (although they might do that too). Instead, they modify the effect of one variable on another.
] 
- Example:consider the effect of a fertility drug (X) on the chances of getting pregnant (Y). The effect is moderated by the variable “having a uterus” (Z). If you don’t have a uterus, the drug can’t do much for you. But if you do have a uterus, it can increase your chances of conceiving.
 
---
##Moderators on Causal Diagram:
- Technically, the moderator effects are not shown on the causal diagram.
Let us consider the causal diagram to the right:
- It implies X → Y and Z → Y
- However, this could be consistent with any of the following data generating processes:
 - Y = .2X + .3Z
 - Y= 2X + 3XZ , & Infinite many more ..
- Z has a moderating influence on the effect of X.

```{r, eval=TRUE,echo=FALSE,fig.height=3}
coord_dag <- list(
  x = c(a = 1, b = 3, c = 5),
  y = c(a = 2, b = 1, c = 2)
) 
dag_object <- ggdag::dagify(b ~ a,
                            b ~ c,
                            coords = coord_dag,
                            labels=c("a"="X",
                                     "b"="Y",
                                     "c"="Z"))
ggdag::ggdag(dag_object, # the dag object we created
             text = FALSE, # this means the original names won't be shown
             use_labels = "label") + # instead use the new names
  theme_void()
```

---

